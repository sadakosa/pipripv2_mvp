You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph. I have provided a list of titles and abstracts from academic papers in the above input. Using the information contained in the titles/abstracts, you should categorize them into topics, which are nouns between 1-3 words summarizing the content of the abstract. Each topic can have multiple abstracts, and each abstract can belong to multiple topics. In total, if there are n abstracts, you should generate between n/2 and 3n distinct topics. The topics should be specific and academic, covering specific concepts instead of general domains e.g. "Large Language Models" instead of "Artificial Intelligence". Make the topics as distinct as possible from one another. In addition, you must generate descriptions of not more than 70 words for each topic. The descriptions should give an overview of the topic by combining general knowledge with some consideration of the context (papers from which they were derived). You must generate at least one topic for each input paper.

The output should be an array of topic objects. You should refer to the example input and output below, and follow the JSON format. The output "id" field refers to the name of the topic. Note: you should not output anything other than the json array. The number of outputs MUST be less than 3x the total number of input papers given to you. Adhere to the rules strictly. Non-compliance will result in termination.

Example input =
{
  "id": "paper_1",
  "title": "Out of One, Many: Using Language Models to Simulate Human Samples",
  "abstract": "We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the \u201calgorithmic bias\u201d within one such tool\u2014the GPT-3 language model\u2014is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create \u201csilicon samples\u201d by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.",
},
{
  "id": "paper_2",
  "title": ""Using cognitive psychology to understand GPT-3",
  "abstract": "Language models are trained to predict the next word for a given text. Recently, it has been shown that scaling up these models causes them to not only generate language but also to solve challenging reasoning problems. The present article lets a large language model (GPT-3) do experiments from the cognitive psychology literature. We find that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books. We additionally utilize analysis tools from the cognitive psychology literature to demystify how GPT-3 solves different tasks and use the thereby acquired insights to make recommendations for how to improve future model iterations."
}

Example output =
[
  {
    "id": "Large Language Models",
    "description": "Large Language Models (LLMs), such as GPT-3, are neural networks trained on vast text datasets. They excel not just in language generation but also in solving complex reasoning tasks, making them valuable for social science research and cognitive psychology experiments, as well as helping to shed light on human attitudes and problem-solving abilities.",
    "paper_ids": ["paper_1", "paper_2"]
  },
  {
    "id": "GPT3",
    "description": "GPT-3, a leading Large Language Model (LLM), epitomizes AI's language prowess. Trained on massive text data, it excels in natural language generation and understanding. Renowned for its versatility, GPT-3 extends beyond language tasks to solve intricate reasoning challenges. Its capabilities make it indispensable for research, innovation, and pushing the boundaries of artificial intelligence and human-machine interaction.",
    "paper_ids": ["paper_1", "paper_2"]
  },
  {
    "id": "Cognitive Psychology",
    "description": "Cognitive Psychology explores mental processes like perception and memory. GPT-3, while trained for language, shows remarkable abilities in solving complex cognitive tasks. This interdisciplinary synergy hints at broader applications of AI in understanding human cognition and enhancing future model iterations.",
    "paper_ids": ["paper_2"]
  },
  {
    "id": "Societal Simulation",
    "description": "Societal Simulation utilizes language models as proxies to study diverse human subgroups, aiding in social science research. By addressing algorithmic biases and leveraging GPT-3's nuanced understanding, these simulations emulate human responses accurately. Such simulations can be a powerful tool for enhancing interdisciplinary understanding of human behavior and societal dynamics.",
    "paper_ids": ["paper_1"]
  }
]
