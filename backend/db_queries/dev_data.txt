CREATE (n:Paper { id: 'Generative Agents: Interactive Simulacra of Human Behavior', arxiv_id: '2304.03442', url: 'https://www.semanticscholar.org/paper/5278a8eb2ba2429d4029745caf4e661080073c81', citation_count: '639', title: 'Generative Agents: Interactive Simulacra of Human Behavior', abstract: 'Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent�s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine�s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture�observation, planning, and reflection�each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.', authors: ['J. Park', "Joseph C. O\\'Brien", 'Carrie J. Cai', 'M. Morris', 'Percy Liang', 'Michael S. Bernstein'], publication_date: '2023-04-07'});
CREATE (n:Paper { id: 'Mirages: On Anthropomorphism in Dialogue Systems', arxiv_id: '2305.09800', url: 'https://www.semanticscholar.org/paper/9379d519b8ddfa194ef6f575127451e5016e1803', citation_count: '11', title: 'Mirages: On Anthropomorphism in Dialogue Systems', abstract: 'Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism may be inevitable due to the choice of medium, conscious and unconscious design choices can guide users to personify such systems to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have investigated the factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be explored. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, including reinforcing gender stereotypes and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.', authors: ['Gavin Abercrombie', 'A. C. Curry', 'Tanvi Dinkar', 'Zeerak Talat'], publication_date: '2023-05-16'});
MATCH (n1:Paper {id: 'Mirages: On Anthropomorphism in Dialogue Systems'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study', arxiv_id: '', url: 'https://www.semanticscholar.org/paper/0ffd57884d7957f6b5634b9fa24843dc3759668f', citation_count: '72', title: 'Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study', abstract: 'Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) research. Motivated by this, we explore the potential of large language models (LLMs) in generating synthetic user research data. We use OpenAI�s GPT-3 model to generate open-ended questionnaire responses about experiencing video games as art, a topic not tractable with traditional computational user models. We test whether synthetic responses can be distinguished from real responses, analyze errors of synthetic data, and investigate content similarities between synthetic and real data. We conclude that GPT-3 can, in this context, yield believable accounts of HCI experiences. Given the low cost and high speed of LLM data generation, synthetic data should be useful in ideating and piloting new experiments, although any findings must obviously always be validated with real data. The results also raise concerns: if employed by malicious users of crowdsourcing services, LLMs may make crowdsourcing of self-report data fundamentally unreliable.', authors: ['Perttu Hamalainen', 'Mikke Tavast', 'Anton Kunnari'], publication_date: '2023-04-19'});
MATCH (n1:Paper {id: 'Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Sparks of Artificial General Intelligence: Early experiments with GPT-4', arxiv_id: '2303.12712', url: 'https://www.semanticscholar.org/paper/574beee702be3856d60aa482ec725168fe64fc99', citation_count: '1586', title: 'Sparks of Artificial General Intelligence: Early experiments with GPT-4', abstract: 'Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.', authors: ['Sebastien Bubeck', 'Varun Chandrasekaran', 'Ronen Eldan', 'J. Gehrke', 'Eric Horvitz', 'Ece Kamar', 'Peter Lee', 'Y. Lee', 'Yuan-Fang Li', 'Scott M. Lundberg', 'Harsha Nori', 'Hamid Palangi', 'Marco Tulio Ribeiro', 'Yi Zhang'], publication_date: '2023-03-22'});
MATCH (n1:Paper {id: 'Sparks of Artificial General Intelligence: Early experiments with GPT-4'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Model Sketching: Centering Concepts in Early-Stage Machine Learning Model Design', arxiv_id: '2303.02884', url: 'https://www.semanticscholar.org/paper/5e684f5015dfd9e4ee773d39336cc219276eb79d', citation_count: '12', title: 'Model Sketching: Centering Concepts in Early-Stage Machine Learning Model Design', abstract: 'Machine learning practitioners often end up tunneling on low-level technical details like model architectures and performance metrics. Could early model development instead focus on high-level questions of which factors a model ought to pay attention to? Inspired by the practice of sketching in design, which distills ideas to their minimal representation, we introduce model sketching: a technical framework for iteratively and rapidly authoring functional approximations of a machine learning model�s decision-making logic. Model sketching refocuses practitioner attention on composing high-level, human-understandable concepts that the model is expected to reason over (e.g., profanity, racism, or sarcasm in a content moderation task) using zero-shot concept instantiation. In an evaluation with 17 ML practitioners, model sketching reframed thinking from implementation to higher-level exploration, prompted iteration on a broader range of model designs, and helped identify gaps in the problem formulation�all in a fraction of the time ordinarily required to build a model.', authors: ['Michelle S. Lam', 'Zixian Ma', 'Anne Li', 'Izequiel Freitas', 'Dakuo Wang', 'J. Landay', 'Michael S. Bernstein'], publication_date: '2023-03-06'});
MATCH (n1:Paper {id: 'Model Sketching: Centering Concepts in Early-Stage Machine Learning Model Design'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?', arxiv_id: '2301.07543', url: 'https://www.semanticscholar.org/paper/898842b1c1fb4689ffa2f20c0836961a962b7691', citation_count: '89', title: 'Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?', abstract: 'Newly-developed large language models (LLM) -- because of how they are trained and designed -- are implicit computational models of humans -- a homo silicus. These models can be used the same way economists use homo economicus: they can be given endowments, information, preferences, and so on and then their behavior can be explored in scenarios via simulation. I demonstrate this approach using OpenAI's GPT3 with experiments derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986) and Samuelson and Zeckhauser (1988). The findings are qualitatively similar to the original results, but it is also trivially easy to try variations that offer fresh insights. Departing from the traditional laboratory paradigm, I also create a hiring scenario where an employer faces applicants that differ in experience and wage ask and then analyze how a minimum wage affects realized wages and the extent of labor-labor substitution.', authors: ['J. Horton'], publication_date: '2023-01-18'});
MATCH (n1:Paper {id: 'Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP', arxiv_id: '2212.14024', url: 'https://www.semanticscholar.org/paper/03532123ccffae8d411264320e8a5ae2b6eddea0', citation_count: '131', title: 'Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP', abstract: 'Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple"retrieve-then-read"pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp', authors: ['O. Khattab', 'Keshav Santhanam', 'Xiang Lisa Li', 'David Leo Wright Hall', 'Percy Liang', 'Christopher Potts', 'M. Zaharia'], publication_date: '2022-12-28'});
MATCH (n1:Paper {id: 'Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence', arxiv_id: '2210.07109', url: 'https://www.semanticscholar.org/paper/22d34b881d64523da54f13d01fc3c6d93a8412e3', citation_count: '23', title: 'Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence', abstract: 'AI researchers have posited Dungeons and Dragons (D&D) as a challenge problem to test systems on various language-related capabilities. In this paper, we frame D&D specifically as a dialogue system challenge, where the tasks are to both generate the next conversational turn in the game and predict the state of the game given the dialogue history. We create a gameplay dataset consisting of nearly 900 games, with a total of 7,000 players, 800,000 dialogue turns, 500,000 dice rolls, and 58 million words. We automatically annotate the data with partial state information about the game play. We train a large language model (LM) to generate the next game turn, conditioning it on different information. The LM can respond as a particular character or as the player who runs the game�i.e., the Dungeon Master (DM). It is trained to produce dialogue that is either in-character (roleplaying in the fictional world) or out-of-character (discussing rules or strategy). We perform a human evaluation to determine what factors make the generated output plausible and interesting. We further perform an automatic evaluation to determine how well the model can predict the game state given the history and examine how well tracking the game state improves its ability to produce plausible conversational output.', authors: ['Chris Callison-Burch', 'Gaurav Singh Tomar', 'Lara J. Martin', 'Daphne Ippolito', 'Suma Bailis', 'D. Reitter'], publication_date: '2022-10-13'});
MATCH (n1:Paper {id: 'Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Socially situated artificial intelligence enables learning from human interaction', arxiv_id: '', url: 'https://www.semanticscholar.org/paper/c3d3889cdf086e3104824c4cd05c5d09b1396b1f', citation_count: '32', title: 'Socially situated artificial intelligence enables learning from human interaction', abstract: 'Significance Humans have long demonstrated an ability to learn from interactions with others. However, artificial intelligence (AI) agents learn in social isolation. To create intelligent systems that understand more than a fixed slice of the world, our article formalizes socially situated AI�a framework that enables agents to interact with people as they simultaneously learn new concepts about the world around them. Using our framework, we deploy a field experiment on a photo-sharing social network where our agent interacts with hundreds of thousands of people to learn concepts about the visual world. We combine advances in deep learning, computer vision, natural language processing, and human�computer Interaction to deliver a human-centered AI that learns from interactions with people in social environments.', authors: ['Ranjay Krishna', 'Donsuk Lee', 'Li Fei-Fei', 'Michael S. Bernstein'], publication_date: '2022-09-19'});
MATCH (n1:Paper {id: 'Socially situated artificial intelligence enables learning from human interaction'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Out of One, Many: Using Language Models to Simulate Human Samples', arxiv_id: '2209.06899', url: 'https://www.semanticscholar.org/paper/f4e612658bde9db88abfd455b99f181fdc536996', citation_count: '222', title: 'Out of One, Many: Using Language Models to Simulate Human Samples', abstract: 'Abstract We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the �algorithmic bias� within one such tool�the GPT-3 language model�is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create �silicon samples� by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.', authors: ['Lisa P. Argyle', 'E. Busby', 'Nancy Fulda', 'Joshua R Gubler', 'Christopher Rytting', 'Taylor Sorensen', 'D. Wingate'], publication_date: '2022-09-14'});
MATCH (n1:Paper {id: 'Out of One, Many: Using Language Models to Simulate Human Samples'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Social Simulacra: Creating Populated Prototypes for Social Computing Systems', arxiv_id: '2208.04024', url: 'https://www.semanticscholar.org/paper/49b499598a8864eee55ab264fc16a5bf8d2f87ef', citation_count: '128', title: 'Social Simulacra: Creating Populated Prototypes for Social Computing Systems', abstract: 'Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designer�s description of a community�s design�goal, rules, and member personas�and produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of �what if?� scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models� training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra.', authors: ['J. Park', 'Lindsay Popowski', 'Carrie J. Cai', 'M. Morris', 'Percy Liang', 'Michael S. Bernstein'], publication_date: '2022-08-08'});
MATCH (n1:Paper {id: 'Social Simulacra: Creating Populated Prototypes for Social Computing Systems'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Inner Monologue: Embodied Reasoning through Planning with Language Models', arxiv_id: '2207.05608', url: 'https://www.semanticscholar.org/paper/f3cf71c51b882fe3111d71c4bf104297d38197f8', citation_count: '480', title: 'Inner Monologue: Embodied Reasoning through Planning with Language Models', abstract: 'Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.', authors: ['Wenlong Huang', 'F. Xia', 'Ted Xiao', 'Harris Chan', 'Jacky Liang', 'Peter R. Florence', 'Andy Zeng', 'Jonathan Tompson', 'Igor Mordatch', 'Yevgen Chebotar', 'P. Sermanet', 'Noah Brown', 'Tomas Jackson', 'Linda Luu', 'S. Levine', 'Karol Hausman', 'Brian Ichter'], publication_date: '2022-07-12'});
MATCH (n1:Paper {id: 'Inner Monologue: Embodied Reasoning through Planning with Language Models'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Using cognitive psychology to understand GPT-3', arxiv_id: '2206.14576', url: 'https://www.semanticscholar.org/paper/fa3609e00f9f422a309c621a35394c4a38f88687', citation_count: '203', title: 'Using cognitive psychology to understand GPT-3', abstract: 'Significance Language models are trained to predict the next word for a given text. Recently, it has been shown that scaling up these models causes them to not only generate language but also to solve challenging reasoning problems. The present article lets a large language model (GPT-3) do experiments from the cognitive psychology literature. We find that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books. We additionally utilize analysis tools from the cognitive psychology literature to demystify how GPT-3 solves different tasks and use the thereby acquired insights to make recommendations for how to improve future model iterations.', authors: ['Marcel Binz', 'Eric Schulz'], publication_date: '2022-06-21'});
MATCH (n1:Paper {id: 'Using cognitive psychology to understand GPT-3'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'PromptMaker: Prompt-based Prototyping with Large Language Models', arxiv_id: '', url: 'https://www.semanticscholar.org/paper/5bc61019771a9fe2a12cc41bad1d9ae4222a152c', citation_count: '58', title: 'PromptMaker: Prompt-based Prototyping with Large�Language�Models', abstract: 'Prototyping is notoriously difficult to do with machine learning (ML), but recent advances in large language models may lower the barriers to people prototyping with ML, through the use of natural language prompts. This case study reports on the real-world experiences of industry professionals (e.g. designers, program managers, front-end developers) prototyping new ML-powered feature ideas via prompt-based prototyping. Through interviews with eleven practitioners during a three-week sprint and a workshop, we find that prompt-based prototyping reduced barriers of access by substantially broadening who can prototype with ML, sped up the prototyping process, and grounded communication between collaborators. Yet, it also introduced new challenges, such as the need to reverse-engineer prompt designs, source example data, and debug and evaluate prompt effectiveness. Taken together, this case study provides important implications that lay the groundwork toward a new future of prototyping with ML.', authors: ['Ellen Jiang', 'Kristen Olson', 'Edwin Toh', 'A. Molina', 'Aaron Donsbach', 'Michael Terry', 'Carrie J. Cai'], publication_date: '2022-04-27'});
MATCH (n1:Paper {id: 'PromptMaker: Prompt-based Prototyping with Large Language Models'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Opal: Multimodal Image Generation for News Illustration', arxiv_id: '2204.09007', url: 'https://www.semanticscholar.org/paper/2a3e997957a023ae7c4f43a92ad57a63edfc9ca1', citation_count: '57', title: 'Opal: Multimodal Image Generation for News Illustration', abstract: 'Advances in multimodal AI have presented people with powerful ways to create images from text. Recent work has shown that text-to-image generations are able to represent a broad range of subjects and artistic styles. However, finding the right visual language for text prompts is difficult. In this paper, we address this challenge with Opal, a system that produces text-to-image generations for news illustration. Given an article, Opal guides users through a structured search for visual concepts and provides a pipeline allowing users to generate illustrations based on an article�s tone, keywords, and related artistic styles. Our evaluation shows that Opal efficiently generates diverse sets of news illustrations, visual assets, and concept ideas. Users with Opal generated two times more usable results than users without. We discuss how structured exploration can help users better understand the capabilities of human AI co-creative systems.', authors: ['Vivian Liu', 'Han Qiao', 'Lydia B. Chilton'], publication_date: '2022-04-19'});
MATCH (n1:Paper {id: 'Opal: Multimodal Image Generation for News Illustration'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'PromptChainer: Chaining Large Language Model Prompts through Visual Programming', arxiv_id: '2203.06566', url: 'https://www.semanticscholar.org/paper/0f733817e82026f7c29909a51cb4df7d2685f0e7', citation_count: '130', title: 'PromptChainer: Chaining Large Language Model Prompts through Visual Programming', abstract: 'While LLMs have made it possible to rapidly prototype new ML functionalities, many real-world applications involve complex tasks that cannot be easily handled via a single run of an LLM. Recent work has found that chaining multiple LLM runs together (with the output of one step being the input to the next) can help users accomplish these more complex tasks, and in a way that is perceived to be more transparent and controllable. However, it remains unknown what users need when authoring their own LLM chains � a key step to lowering the barriers for non-AI-experts to prototype AI-infused applications. In this work, we explore the LLM chain authoring process. We find from pilot studies that users need support transforming data between steps of a chain, as well as debugging the chain at multiple granularities. To address these needs, we designed PromptChainer, an interactive interface for visually programming chains. Through case studies with four designers and developers, we show that PromptChainer supports building prototypes for a range of applications, and conclude with open questions on scaling chains to even more complex tasks, as well as supporting low-fi chain prototyping.', authors: ['Tongshuang Sherry Wu', 'Ellen Jiang', 'Aaron Donsbach', 'J. Gray', 'A. Molina', 'Michael Terry', 'Carrie J. Cai'], publication_date: '2022-03-13'});
MATCH (n1:Paper {id: 'PromptChainer: Chaining Large Language Model Prompts through Visual Programming'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Training language models to follow instructions with human feedback', arxiv_id: '2203.02155', url: 'https://www.semanticscholar.org/paper/d766bffc357127e0dc86dd69561d5aeb520d6f4c', citation_count: '5886', title: 'Training language models to follow instructions with human feedback', abstract: 'Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.', authors: ['Long Ouyang', 'Jeff Wu', 'Xu Jiang', 'Diogo Almeida', 'Carroll L. Wainwright', 'Pamela Mishkin', 'Chong Zhang', 'Sandhini Agarwal', 'Katarina Slama', 'Alex Ray', 'John Schulman', 'Jacob Hilton', 'Fraser Kelton', 'Luke E. Miller', 'Maddie Simens', 'Amanda Askell', 'P. Welinder', 'P. Christiano', 'J. Leike', 'Ryan J. Lowe'], publication_date: '2022-03-04'});
MATCH (n1:Paper {id: 'Training language models to follow instructions with human feedback'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Chain of Thought Prompting Elicits Reasoning in Large Language Models', arxiv_id: '2201.11903', url: 'https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5', citation_count: '3479', title: 'Chain of Thought Prompting Elicits Reasoning in Large Language Models', abstract: 'We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.', authors: ['Jason Wei', 'Xuezhi Wang', 'Dale Schuurmans', 'Maarten Bosma', 'E. Chi', 'F. Xia', 'Quoc Le', 'Denny Zhou'], publication_date: '2022-01-28'});
MATCH (n1:Paper {id: 'Chain of Thought Prompting Elicits Reasoning in Large Language Models'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts', arxiv_id: '2110.01691', url: 'https://www.semanticscholar.org/paper/d3640eb3b542eaf36fee2261f037a6bf0d8eac9c', citation_count: '232', title: 'AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts', abstract: 'Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by �unit-testing� sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications.', authors: ['Tongshuang Sherry Wu', 'Michael Terry', 'Carrie J. Cai'], publication_date: '2021-10-04'});
MATCH (n1:Paper {id: 'AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Recursively Summarizing Books with Human Feedback', arxiv_id: '2109.10862', url: 'https://www.semanticscholar.org/paper/a6fdb277d0a4b09899f802bda3359f5c2021a156', citation_count: '203', title: 'Recursively Summarizing Books with Human Feedback', abstract: 'A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ( of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.', authors: ['Jeff Wu', 'Long Ouyang', 'Daniel M. Ziegler', 'Nissan Stiennon', 'Ryan Lowe', 'J. Leike', 'P. Christiano'], publication_date: '2021-09-22'});
MATCH (n1:Paper {id: 'Recursively Summarizing Books with Human Feedback'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);
CREATE (n:Paper { id: 'Human-Centered AI', arxiv_id: '', url: 'https://www.semanticscholar.org/paper/264b9b136889da3b4d7e50ef58c77678b35dc3e0', citation_count: '156', title: 'Human-Centered AI', abstract: 'We identify three specific areas of focus to advance human-centered AI:� Designers and systems must understand the context of use and sense changes over time: Successful AI Engineering depends on the team�s ability to identify and articulate the desired system outcome and understand human and contextual factors affecting the outcome. The system itself must be able to learn when shifts in context have occurred. What are the best ways to maintain clarity around operational intent and mechanisms for adapting and evolving systems based on dynamic contexts and user needs?� Development of tools, processes, and practices to scope and facilitate human-machine teaming: Implementation of AI systems entails high levels of interdependence between human and machine. Adoption of AI systems requires the primary users to interact with and understand systems, gaining appropriate levels of trust. Every AI system needs to be designed to recognize boundaries and unfamiliar scenarios, and to provide transparency regarding its limitations.� Methods, mechanisms, and mindsets to engage in critical oversight: AI systems learn through data and observations, rather than being explicitly programmed for a deterministic outcome. Critical and reflective oversight by organizations, teams, and individuals that create and use AI systems is needed to uphold ethical principles and proactively consider the risks of bias, misuse, abuse, and unintended consequences through design, development, and ongoing deployment.For each area, we identify ongoing work as well and challenges and opportunities in developing and deploying AI systems with confidence.', authors: ['Hollen Barmer', 'R. Dzombak', 'M. Gaston', 'Vijaykumar Palat', 'F. Redner', 'Carol J. Smith', 'Tanisha Smith'], publication_date: '2021-09-02'});
MATCH (n1:Paper {id: 'Human-Centered AI'}), (n2:Paper {id: 'Generative Agents: Interactive Simulacra of Human Behavior'}) CREATE (n1)-[:is_cited_by]->(n2);

CREATE (n:Topic { id: 'topic_one', description: 'This is the summary of this topic. Really a very good summary'});
CREATE (n:Topic { id: 'topic_two', description: 'This is the summary of this topic. Really a very good summary'});
CREATE (n:Topic { id: 'topic_three', description: 'This is the summary of this topic. Really a very good summary'});