CREATE (n:Topic { id: 'LLM Arithmetic', description: ''});
CREATE (n:Topic { id: 'Web Data', description: ''});
CREATE (n:Topic { id: 'Factual and Ethical Considerations', description: ''});
CREATE (n:Topic { id: 'Model Scaling', description: ''});
CREATE (n:Topic { id: 'Reasoning', description: ''});
CREATE (n:Topic { id: 'Text-to-Image Generation', description: ''});
CREATE (n:Topic { id: 'Subword Units', description: ''});
CREATE (n:Topic { id: 'Neural Machine Translation', description: ''});
CREATE (n:Topic { id: 'Machine Learning Algorithms', description: ''});
CREATE (n:Topic { id: 'Natural Language Processing', description: ''});
CREATE (n:Topic { id: 'Knowledge Representation', description: ''});
CREATE (n:Paper { id: 'NumeroLogic: Number Encoding for Enhanced LLMs\' Numerical Reasoning', arxiv_id: '2404.00459', url: 'https://www.semanticscholar.org/paper/b9750286ba2198a406137e0dfee2d545f0d78c13', citation_count: '0', title: 'NumeroLogic: Number Encoding for Enhanced LLMs\' Numerical Reasoning', abstract: 'Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of"42", we suggest using"{2:42}"as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.', authors: ['Eli Schwartz', 'Leshem Choshen', 'J. Shtok', 'Sivan Doveh', 'Leonid Karlinsky', 'Assaf Arbelle'], publication_date: '2024-03-30'});
CREATE (n:Paper { id: 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', arxiv_id: '2402.14903', url: 'https://www.semanticscholar.org/paper/49c45d2a2773c537804c38d69cde67e00fbad6fe', citation_count: '4', title: 'Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs', abstract: 'Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between tokenizations easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias. In summary, our work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number tokenization-related choices when working towards general models of numerical reasoning.', authors: ['Aaditya K. Singh', 'DJ Strouse'], publication_date: '2024-02-22'});
CREATE (n:Paper { id: 'Positional Description Matters for Transformers Arithmetic', arxiv_id: '2311.14737', url: 'https://www.semanticscholar.org/paper/4726d1dc54851db99c29180127d840bd19f20afc', citation_count: '6', title: 'Positional Description Matters for Transformers Arithmetic', abstract: 'Transformers, central to the successes in modern Natural Language Processing, often falter on arithmetic tasks despite their vast capabilities --which paradoxically include remarkable coding abilities. We observe that a crucial challenge is their naive reliance on positional information to solve arithmetic problems with a small number of digits, leading to poor performance on larger numbers. Herein, we delve deeper into the role of positional encoding, and propose several ways to fix the issue, either by modifying the positional encoding directly, or by modifying the representation of the arithmetic task to leverage standard positional encoding differently. We investigate the value of these modifications for three tasks: (i) classical multiplication, (ii) length extrapolation in addition, and (iii) addition in natural language context. For (i) we train a small model on a small dataset (100M parameters and 300k samples) with remarkable aptitude in (direct, no scratchpad) 15 digits multiplication and essentially perfect up to 12 digits, while usual training in this context would give a model failing at 4 digits multiplication. In the experiments on addition, we use a mere 120k samples to demonstrate: for (ii) extrapolation from 10 digits to testing on 12 digits numbers while usual training would have no extrapolation, and for (iii) almost perfect accuracy up to 5 digits while usual training would be correct only up to 3 digits (which is essentially memorization with a training set of 120k samples).', authors: ['Ruoqi Shen', 'Sebastien Bubeck', 'Ronen Eldan', 'Yin Tat Lee', 'Yuanzhi Li', 'Yi Zhang'], publication_date: '2023-11-22'});
CREATE (n:Paper { id: 'Mistral 7B', arxiv_id: '2310.06825', url: 'https://www.semanticscholar.org/paper/db633c6b1c286c0386f0078d8a2e6224e03a6227', citation_count: '336', title: 'Mistral 7B', abstract: 'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.', authors: ['Albert Qiaochu Jiang', 'Alexandre Sablayrolles', 'A. Mensch', 'Chris Bamford', 'Devendra Singh Chaplot', 'Diego de Las Casas', 'Florian Bressand', 'Gianna Lengyel', 'Guillaume Lample', 'Lucile Saulnier', "L\\'elio Renard Lavaud", 'Marie-Anne Lachaux', 'Pierre Stock', 'Teven Le Scao', 'Thibaut Lavril', 'Thomas Wang', 'Timothee Lacroix', 'William El Sayed'], publication_date: '2023-10-10'});
CREATE (n:Paper { id: 'Teaching Arithmetic to Small Transformers', arxiv_id: '2307.03381', url: 'https://www.semanticscholar.org/paper/0db0af0cd3ceb0531a050a03e6ceb849580ff53b', citation_count: '28', title: 'Teaching Arithmetic to Small Transformers', abstract: 'Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities.', authors: ['Nayoung Lee', 'Kartik K. Sreenivasan', 'Jason D. Lee', 'Kangwook Lee', 'Dimitris Papailiopoulos'], publication_date: '2023-07-07'});
CREATE (n:Paper { id: 'The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only', arxiv_id: '2306.01116', url: 'https://www.semanticscholar.org/paper/7a1e71cb1310c4a873e7a4e54d1a6dab0553adce', citation_count: '398', title: 'The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only', abstract: 'Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.', authors: ['Guilherme Penedo', 'Quentin Malartic', 'Daniel Hesslow', 'Ruxandra-Aimee Cojocaru', 'Alessandro Cappelli', 'Hamza Alobeidli', 'B. Pannier', 'Ebtesam Almazrouei', 'Julien Launay'], publication_date: '2023-06-01'});
CREATE (n:Paper { id: 'GPT-4 Technical Report', arxiv_id: '2303.08774', url: 'https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04', citation_count: '3698', title: 'GPT-4 Technical Report', abstract: 'We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10 of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4\'s performance based on models trained with no more than 1/1,000th the compute of GPT-4.', authors: ['OpenAI Josh Achiam', 'Steven Adler', 'Sandhini Agarwal', 'Lama Ahmad', 'Ilge Akkaya', 'Florencia Leoni Aleman', 'Diogo Almeida', 'Janko Altenschmidt', 'Sam Altman', 'Shyamal Anadkat', 'Red Avila', 'Igor Babuschkin', 'S. Balaji', 'Valerie Balcom', 'Paul Baltescu', 'Haiming Bao', 'Mo Bavarian', 'Jeff Belgum', 'Irwan Bello', 'Jake Berdine', 'Gabriel Bernadett-Shapiro', 'Christopher Berner', 'Lenny Bogdonoff', 'Oleg Boiko', 'Madelaine Boyd', 'Anna-Luisa Brakman', 'Greg Brockman', 'Tim Brooks', 'Miles Brundage', 'Kevin Button', 'Trevor Cai', 'Rosie Campbell', 'Andrew Cann', 'Brittany Carey', 'Chelsea Carlson', 'Rory Carmichael', 'Brooke Chan', 'Che Chang', 'Fotis Chantzis', 'Derek Chen', 'Sully Chen', 'Ruby Chen', 'Jason Chen', 'Mark Chen', 'B. Chess', 'Chester Cho', 'Casey Chu', 'Hyung Won Chung', 'Dave Cummings', 'Jeremiah Currier', 'Yunxing Dai', 'Cory Decareaux', 'Thomas Degry', 'Noah Deutsch', 'Damien Deville', 'Arka Dhar', 'David Dohan', 'Steve Dowling', 'Sheila Dunning', 'Adrien Ecoffet', 'Atty Eleti', 'Tyna Eloundou', 'David Farhi', 'L. Fedus', 'Niko Felix', "Sim\\'on Posada Fishman", 'Juston Forte', 'Isabella Fulford', 'Leo Gao', 'Elie Georges', 'C. Gibson', 'Vik Goel', 'Tarun Gogineni', 'Gabriel Goh', 'Raphael Gontijo-Lopes', 'Jonathan Gordon', 'Morgan Grafstein', 'S. Gray', 'Ryan Greene', 'Joshua Gross', 'S. Gu', 'Yufei Guo', 'Chris Hallacy', 'Jesse Han', 'Jeff Harris', 'Yuchen He', 'Mike Heaton', 'Johannes Heidecke', 'Chris Hesse', 'Alan Hickey', 'Wade Hickey', 'Peter Hoeschele', 'Brandon Houghton', 'Kenny Hsu', 'Shengli Hu', 'Xin Hu', 'Joost Huizinga', 'Shantanu Jain', 'Shawn Jain', 'Joanne Jang', 'Angela Jiang', 'Roger Jiang', 'Haozhun Jin', 'Denny Jin', 'Shino Jomoto', 'Billie Jonn', 'Heewoo Jun', 'Tomer Kaftan', 'Lukasz Kaiser', 'Ali Kamali', 'I. Kanitscheider', 'N. Keskar', 'Tabarak Khan', 'Logan Kilpatrick', 'Jong Wook Kim', 'Christina Kim', 'Yongjik Kim', 'Hendrik Kirchner', 'J. Kiros', 'Matthew Knight', 'Daniel Kokotajlo', 'Lukasz Kondraciuk', 'A. Kondrich', 'Aris Konstantinidis', 'Kyle Kosic', 'Gretchen Krueger', 'Vishal Kuo', 'Michael Lampe', 'Ikai Lan', 'Teddy Lee', 'J. Leike', 'Jade Leung', 'Daniel Levy', 'Chak Ming Li', 'Rachel Lim', 'Molly Lin', 'Stephanie Lin', 'Mateusz Litwin', 'Theresa Lopez', 'Ryan Lowe', 'Patricia Lue', 'A. Makanju', 'Kim Malfacini', 'Sam Manning', 'Todor Markov', 'Yaniv Markovski', 'Bianca Martin', 'Katie Mayer', 'Andrew Mayne', 'Bob McGrew', 'S. McKinney', 'C. McLeavey', 'Paul McMillan', 'Jake McNeil', 'David Medina', 'Aalok Mehta', 'Jacob Menick', 'Luke Metz', 'Andrey Mishchenko', 'Pamela Mishkin', 'Vinnie Monaco', 'Evan Morikawa', 'Daniel P. Mossing', 'Tong Mu', 'Mira Murati', 'O. Murk', "David M\\'ely", 'Ashvin Nair', 'Reiichiro Nakano', 'Rajeev Nayak', 'Arvind Neelakantan', 'Richard Ngo', 'Hyeonwoo Noh', 'Ouyang Long', "Cullen O\\'Keefe", 'J. Pachocki', 'Alex Paino', 'Joe Palermo', 'Ashley Pantuliano', 'Giambattista Parascandolo', 'Joel Parish', 'Emy Parparita', 'Alexandre Passos', 'Mikhail Pavlov', 'Andrew Peng', 'Adam Perelman', 'Filipe de Avila Belbute Peres', 'Michael Petrov', 'Henrique Ponde de Oliveira Pinto', 'Michael Pokorny', 'Michelle Pokrass', 'Vitchyr H. Pong', 'Tolly Powell', 'Alethea Power', 'Boris Power', 'Elizabeth Proehl', 'Raul Puri', 'Alec Radford', 'Jack Rae', 'Aditya Ramesh', 'Cameron Raymond', 'Francis Real', 'Kendra Rimbach', 'Carl Ross', 'Bob Rotsted', 'Henri Roussez', 'Nick Ryder', 'M. Saltarelli', 'Ted Sanders', 'Shibani Santurkar', 'Girish Sastry', 'Heather Schmidt', 'David Schnurr', 'John Schulman', 'Daniel Selsam', 'Kyla Sheppard', 'T. Sherbakov', 'Jessica Shieh', 'S. Shoker', 'Pranav Shyam', 'Szymon Sidor', 'Eric Sigler', 'Maddie Simens', 'Jordan Sitkin', 'Katarina Slama', 'Ian Sohl', 'Benjamin D. Sokolowsky', 'Yang Song', 'Natalie Staudacher', 'F. Such', 'Natalie Summers', 'I. Sutskever', 'Jie Tang', 'N. Tezak', 'Madeleine Thompson', 'Phil Tillet', 'Amin Tootoonchian', 'Elizabeth Tseng', 'Preston Tuggle', 'Nick Turley', 'Jerry Tworek', "Juan Felipe Cer\\'on Uribe", 'Andrea Vallone', 'Arun Vijayvergiya', 'Chelsea Voss', 'Carroll Wainwright', 'Justin Jay Wang', 'Alvin Wang', 'Ben Wang', 'Jonathan Ward', 'Jason Wei', 'CJ Weinmann', 'Akila Welihinda', 'P. Welinder', 'Jiayi Weng', 'Lilian Weng', 'Matt Wiethoff', 'Dave Willner', 'Clemens Winter', 'Samuel Wolrich', 'Hannah Wong', 'Lauren Workman', 'Sherwin Wu', 'Jeff Wu', 'Michael Wu', 'Kai Xiao', 'Tao Xu', 'Sarah Yoo', 'Kevin Yu', 'Qiming Yuan', 'Wojciech Zaremba', 'Rowan Zellers', 'Chong Zhang', 'Marvin Zhang', 'Shengjia Zhao', 'Tianhao Zheng', 'Juntang Zhuang', 'William Zhuk', 'Barret Zoph'], publication_date: '2023-03-15'});
CREATE (n:Paper { id: 'LLaMA: Open and Efficient Foundation Language Models', arxiv_id: '2302.13971', url: 'https://www.semanticscholar.org/paper/57e849d0de13ed5f91d086936296721d4ff75a75', citation_count: '5049', title: 'LLaMA: Open and Efficient Foundation Language Models', abstract: 'We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.', authors: ['Hugo Touvron', 'Thibaut Lavril', 'Gautier Izacard', 'Xavier Martinet', 'Marie-Anne Lachaux', 'Timothee Lacroix', 'Baptiste Roziere', 'Naman Goyal', 'Eric Hambro', 'Faisal Azhar', 'Aurelien Rodriguez', 'Armand Joulin', 'Edouard Grave', 'Guillaume Lample'], publication_date: '2023-02-27'});
CREATE (n:Paper { id: 'PaLM: Scaling Language Modeling with Pathways', arxiv_id: '2204.02311', url: 'https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb', citation_count: '3738', title: 'PaLM: Scaling Language Modeling with Pathways', abstract: 'Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.', authors: ['Aakanksha Chowdhery', 'Sharan Narang', 'Jacob Devlin', 'Maarten Bosma', 'Gaurav Mishra', 'Adam Roberts', 'Paul Barham', 'Hyung Won Chung', 'Charles Sutton', 'Sebastian Gehrmann', 'Parker Schuh', 'Kensen Shi', 'Sasha Tsvyashchenko', 'Joshua Maynez', 'Abhishek Rao', 'Parker Barnes', 'Yi Tay', 'Noam M. Shazeer', 'Vinodkumar Prabhakaran', 'Emily Reif', 'Nan Du', 'B. Hutchinson', 'Reiner Pope', 'James Bradbury', 'Jacob Austin', 'M. Isard', 'Guy Gur-Ari', 'Pengcheng Yin', 'Toju Duke', 'Anselm Levskaya', 'Sanjay Ghemawat', 'Sunipa Dev', 'H. Michalewski', 'Xavier Garcia', 'Vedant Misra', 'Kevin Robinson', 'L. Fedus', 'Denny Zhou', 'Daphne Ippolito', 'D. Luan', 'Hyeontaek Lim', 'Barret Zoph', 'A. Spiridonov', 'Ryan Sepassi', 'David Dohan', 'Shivani Agrawal', 'Mark Omernick', 'Andrew M. Dai', 'Thanumalayan Sankaranarayana Pillai', 'Marie Pellat', 'Aitor Lewkowycz', 'Erica Moreira', 'R. Child', 'Oleksandr Polozov', 'Katherine Lee', 'Zongwei Zhou', 'Xuezhi Wang', 'Brennan Saeta', 'Mark Diaz', 'Orhan Firat', 'Michele Catasta', 'Jason Wei', 'K. Meier-Hellstern', 'D. Eck', 'J. Dean', 'Slav Petrov', 'Noah Fiedel'], publication_date: '2022-04-05'});
CREATE (n:Paper { id: 'Chain of Thought Prompting Elicits Reasoning in Large Language Models', arxiv_id: '2201.11903', url: 'https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5', citation_count: '3480', title: 'Chain of Thought Prompting Elicits Reasoning in Large Language Models', abstract: 'We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.', authors: ['Jason Wei', 'Xuezhi Wang', 'Dale Schuurmans', 'Maarten Bosma', 'E. Chi', 'F. Xia', 'Quoc Le', 'Denny Zhou'], publication_date: '2022-01-28'});
CREATE (n:Paper { id: 'LoRA: Low-Rank Adaptation of Large Language Models', arxiv_id: '2106.09685', url: 'https://www.semanticscholar.org/paper/a8ca46b171467ceb2d7652fbfb67fe701ad86092', citation_count: '3224', title: 'LoRA: Low-Rank Adaptation of Large Language Models', abstract: 'An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.', authors: ['J. E. Hu', 'Yelong Shen', 'Phillip Wallis', 'Zeyuan Allen-Zhu', 'Yuanzhi Li', 'Shean Wang', 'Weizhu Chen'], publication_date: '2021-06-17'});
CREATE (n:Paper { id: 'Measuring Massive Multitask Language Understanding', arxiv_id: '2009.03300', url: 'https://www.semanticscholar.org/paper/814a4f680b9ba6baba23b93499f4b48af1a27678', citation_count: '1257', title: 'Measuring Massive Multitask Language Understanding', abstract: 'We propose a new test to measure a text model\'s multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model\'s academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.', authors: ['Dan Hendrycks', 'Collin Burns', 'Steven Basart', 'Andy Zou', 'Mantas Mazeika', 'D. Song', 'J. Steinhardt'], publication_date: '2020-09-07'});
CREATE (n:Paper { id: 'Aligning AI With Shared Human Values', arxiv_id: '2008.02275', url: 'https://www.semanticscholar.org/paper/65906e6027246ae9e4ecd18d6e019a24505c842e', citation_count: '268', title: 'Aligning AI With Shared Human Values', abstract: 'We show how to assess a language model\'s knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.', authors: ['Dan Hendrycks', 'Collin Burns', 'Steven Basart', 'Andrew Critch', 'J. Li', 'D. Song', 'J. Steinhardt'], publication_date: '2020-08-05'});
CREATE (n:Paper { id: 'Neural Machine Translation of Rare Words with Subword Units', arxiv_id: '1508.07909', url: 'https://www.semanticscholar.org/paper/1518039b5001f1836565215eb047526b3ac7f462', citation_count: '6723', title: 'Neural Machine Translation of Rare Words with Subword Units', abstract: 'Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.', authors: ['Rico Sennrich', 'B. Haddow', 'Alexandra Birch'], publication_date: '2015-08-31'});
CREATE (n:Paper { id: 'A new algorithm for data compression', arxiv_id: '', url: 'https://www.semanticscholar.org/paper/1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8', citation_count: '737', title: 'A new algorithm for data compression', abstract: 'Data compression is becoming increasingly important as a way to stretch disk space and speed up data transfers. This article describes a simple general-purpose data compression algo-rithm, called Byte Pair Encoding (BPE), which provides almost as much compression as the popular Lempel, Ziv', authors: ['Philip Gage'], publication_date: '1994-02-01'});
CREATE (n:Paper { id: 'Improving Image Generation with Better Captions', arxiv_id: '', url: 'https://www.semanticscholar.org/paper/cfee1826dd4743eab44c6e27a0cc5970effa4d80', citation_count: '235', title: 'Improving Image Generation with Better Captions', abstract: 'We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions. Existing text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. We hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. We address this by training a bespoke image captioner and use it to recaption the training dataset. We then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, we use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.', authors: ['James Betker', 'Gabriel Goh', 'Li Jing', '+. TimBrooks', 'Jianfeng Wang', 'Linjie Li', '+. LongOuyang', '+. JuntangZhuang', '+. JoyceLee', '+. YufeiGuo', '+. WesamManassra', '+. PrafullaDhariwal', '+. CaseyChu', '+. YunxinJiao', 'Aditya Ramesh'], publication_date: ''});
MATCH (n1:Paper {id: 'b9750286ba2198a406137e0dfee2d545f0d78c13'}), (n2:Topic {id: 'LLM Arithmetic'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: '49c45d2a2773c537804c38d69cde67e00fbad6fe'}), (n2:Topic {id: 'LLM Arithmetic'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: '4726d1dc54851db99c29180127d840bd19f20afc'}), (n2:Topic {id: 'LLM Arithmetic'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: '0db0af0cd3ceb0531a050a03e6ceb849580ff53b'}), (n2:Topic {id: 'LLM Arithmetic'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: 'db633c6b1c286c0386f0078d8a2e6224e03a6227'}), (n2:Topic {id: 'Web Data'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: '163b4d6a79a5b19af88b8585456363340d9efd04'}), (n2:Topic {id: 'Factual and Ethical Considerations'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: '57e849d0de13ed5f91d086936296721d4ff75a75'}), (n2:Topic {id: 'Model Scaling'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: '094ff971d6a8b8ff870946c9b3ce5aa173617bfb'}), (n2:Topic {id: 'Model Scaling'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: '1b6e810ce0afd0dd093f789d2b2742d047e316d5'}), (n2:Topic {id: 'Reasoning'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: '65906e6027246ae9e4ecd18d6e019a24505c842e'}), (n2:Topic {id: 'Reasoning'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: '814a4f680b9ba6baba23b93499f4b48af1a27678'}), (n2:Topic {id: 'Text-to-Image Generation'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: '1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8'}), (n2:Topic {id: 'Text-to-Image Generation'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: 'cfee1826dd4743eab44c6e27a0cc5970effa4d80'}), (n2:Topic {id: 'Text-to-Image Generation'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: '1518039b5001f1836565215eb047526b3ac7f462'}), (n2:Topic {id: 'Subword Units'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Paper {id: '1518039b5001f1836565215eb047526b3ac7f462'}), (n2:Topic {id: 'Neural Machine Translation'}) CREATE (n1)-[:relates_to]->(n2);
MATCH (n1:Topic {id: 'Reasoning'}), (n2:Topic {id: 'LLM Arithmetic'}) CREATE (n1)-[:application_in]->(n2);
MATCH (n1:Topic {id: 'Reasoning'}), (n2:Topic {id: 'Factual and Ethical Considerations'}) CREATE (n1)-[:focus_of]->(n2);
MATCH (n1:Topic {id: 'LLM Arithmetic'}), (n2:Topic {id: 'Model Scaling'}) CREATE (n1)-[:benefits_from]->(n2);
MATCH (n1:Topic {id: 'Web Data'}), (n2:Topic {id: 'Model Scaling'}) CREATE (n1)-[:enables]->(n2);
MATCH (n1:Topic {id: 'Model Scaling'}), (n2:Topic {id: 'Reasoning'}) CREATE (n1)-[:improves]->(n2);
MATCH (n1:Topic {id: 'Text-to-Image Generation'}), (n2:Topic {id: 'Subword Units'}) CREATE (n1)-[:uses]->(n2);
MATCH (n1:Topic {id: 'Text-to-Image Generation'}), (n2:Topic {id: 'Reasoning'}) CREATE (n1)-[:involves]->(n2);
MATCH (n1:Topic {id: 'Machine Learning Algorithms'}), (n2:Topic {id: 'Natural Language Processing'}) CREATE (n1)-[:includes]->(n2);
MATCH (n1:Topic {id: 'Machine Learning Algorithms'}), (n2:Topic {id: 'Knowledge Representation'}) CREATE (n1)-[:builds]->(n2);
MATCH (n1:Topic {id: 'Natural Language Processing'}), (n2:Topic {id: 'Knowledge Representation'}) CREATE (n1)-[:populates]->(n2);
